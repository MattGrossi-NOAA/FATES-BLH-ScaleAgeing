{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047c2e7c",
   "metadata": {},
   "source": [
    "# Chapter 2 - Inference Model on Scale Images\n",
    "Here we will show how to load a pretrianed model on inference on a folder of images.\n",
    "\n",
    "## Dataset and Dataloader\n",
    "First we define a dataset that is used to find and load the images.  This dataloader only returns the images and the image names and not any labels or other information since this is used for inference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1c7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class FishTestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "\n",
    "        # Get the directory dataset images\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        # Get the transform methods\n",
    "        self.transforms = transform\n",
    "\n",
    "\n",
    "        # Image Name\n",
    "        self.image_name = [f for f in listdir(image_dir) if isfile(join(image_dir, f))]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, str(self.image_name[index]))\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, self.image_name[index]\n",
    "        \n",
    "data_dir = 'cropped'\n",
    "data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "test_dataset = FishTestDataset( data_dir, data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f18e1f",
   "metadata": {},
   "source": [
    "## Create and load model\n",
    "Here we use a simple resnet18 classification architecture from pytorch for our model.  We have to change provide the desired number of classes since the default is 1000 classes (for imagenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b628985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(num_classes = 5)\n",
    "\n",
    "# Load model - TODO\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "model.eval()    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eee7f6",
   "metadata": {},
   "source": [
    "## Load images and Inference\n",
    "\n",
    "Here we use our dataloader to load the images by batches (of size 24) and input them into the model for inference.  Also, we output the results to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988500ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "output_path = \"inference_results.csv\"\n",
    "file = open(output_path, 'w')\n",
    "file.write(\"Image Name, Predicted Age\\n\")\n",
    "\n",
    "for images, img_path in tqdm(test_loader):\n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "    outputs = torch.squeeze(outputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    for i in range(preds.shape[0]):\n",
    "        age = str(preds[i])\n",
    "        if(preds[i] ==4):\n",
    "            age = \"4+\"\n",
    "        file.write(\"%s,%s\\n\"%(img_path[i],age))\n",
    "file.close()"
   ]
  },
  {
   "attachments": {
    "6a5d61e3-2d68-4cd4-a960-d300f9c886be.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "805ecbf6",
   "metadata": {},
   "source": [
    "The resulting csv file should look something like this.\n",
    "\n",
    "![image.png](attachment:6a5d61e3-2d68-4cd4-a960-d300f9c886be.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c04670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
